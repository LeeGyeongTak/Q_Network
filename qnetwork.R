


source("nn.bp.R")
source("nn.ff2.R")
source("relu.R")
source("sigm.R")


coord<-function(state){
  re_index<-which(state==1)
  xx<-ceiling(re_index/ 10) ## 행
  yy<-re_index %% 10  ## 열
  yy<-ifelse(yy ==0,10,yy)
  c(xx,yy)
}
move<-function(x,action){
  
  if(action == "left"){
    if(x[2]-1<1){
      x
    }else{
      x[2]<-x[2]-1
      x
    }
  }
  if(action == "right"){
    if(x[2]+1>ncol(stm)){
      x
    }else{
      x[2]<-x[2]+1
      x
    }
  }
  if(action == "up"){
    if(x[1]-1<1){
      x
    }else{
      x[1]<-x[1]-1
      x
    }
  }
  if(action == "down"){
    if(x[1]+1>nrow(stm)){
      x
    }else{
      x[1]<-x[1]+1
      x
    }
  }
  x
}
next_where<-function(index){ 
 zero<-rep(0,100)
 zero[index]<-1
 zero
  
  
}


#######state matrix 
stm<-matrix(1:100,ncol=10,nrow=10,byrow=T)

return_reward<-function(state,current_state){
  re_index<-which(state==1)
  
  if(  re_index==100){
    reward<- 5# episode end
    done<-T
  }
  else if(re_index==12 |re_index==42|re_index==44|re_index==45    |
          re_index==68|re_index==72|re_index==80){
    reward<- -2
    done<-F
  }else{
    reward <- -1
    done<-F
  }
  if(re_index==which(current_state==1)){
    reward<-reward*2
  }
  xx<-ceiling(re_index/ 10) ## row
  yy<-re_index %% 10  ## col
  yy<-ifelse(yy ==0,10,yy)
  reward_weight<-sqrt(162)-sqrt((yy-10)^2+(xx-10)^2) #weigthed reward by distance from current state to goal
  reward<-reward+reward_weight*0.05
  c(reward,done)
  
}
action<-c("left","right","down","up")
## 10 x 10  frozen lake problem
# S : start, F : Frozen, H : Hole, G : Goal 
# SFFFF|FFFFF
# FHFFF|FFFFF
# FFFFF|FFFFF
# FFFFF|FFFFF
# FHFHH|FFFFF
# FFFFF|FFFFF
# FFFFF|FFHFF
# FHFFF|FFFFH
# FFFFF|FFFFF
# FFFFF|FFFFG
### initialize neural network

{
  
  input_dim<-100
  hidden<-c(30)
  output_dim<-4
  size <- c(input_dim, hidden, output_dim)
  activationfun<-"relu"
  output<-"linear"
  
  momentum<-0
  learningrate_scale<-1
  hidden_dropout = 0
  visible_dropout = 0
  numepochs = 10
  learningrate<-0.01
  
  vW <- list()
  vB <- list()
  W <- list()
  B <- list()
  
  
  
  for (i in 2:length(size)) {
    W[[i - 1]] <- matrix(runif(size[i] * size[i - 1], 
                               min = -0.1, max = 0.1), c(size[i], size[i - 1]))
    B[[i - 1]] <- runif(size[i], min = -0.1, max = 0.1)
    vW[[i - 1]] <- matrix(rep(0, size[i] * size[i - 1]), 
                          c(size[i], size[i - 1]))
    vB[[i - 1]] <- rep(0, size[i])
  }
  qn1<- list(input_dim = input_dim, output_dim = output_dim, 
             hidden = hidden, size = size, activationfun = activationfun, 
             learningrate = learningrate, momentum = momentum, learningrate_scale = learningrate_scale, 
             hidden_dropout = hidden_dropout, visible_dropout = visible_dropout, 
             output = output, W = W, vW = vW, B = B, vB = vB)
  
}




init_data<-c(1,rep(0,99))
dis_f<-0.99
reward_list<-c()
final_action_list<-list()
step_list<-c()
q_table<-list()

for(i in 1:10000){
  total_r<-0
  episode_done<-0
  qn1<-nn.ff2(qn1,t(init_data))
  step<-1
  action_list<-NULL
  st<-c(1,1)
  
  
  while(episode_done==0){
    
    if(step >1){
 
      qn1<-nn.ff2(qn1,t(next_state))
      action_index<-which.max(qn1$post[[length(size)]])
      current_state<-next_state
      
    }else{
      current_state<-init_data
      action_index<-which.max(qn1$post[[length(size)]])
    }
    
   
    
    th<-1/(i/50+10)
    if(runif(1) < th){ ## e-greedy search
      action_index<-sample(1:4,1)
      next_action<-  action[action_index]
    }else{
      next_action<-action[action_index] ### action
    }
    
    action_list<-c(action_list,next_action)
    st<-move(st,next_action)
    state_index<-stm[st[1],st[2]]
  
    next_state<-next_where(state_index)
    
    #### target value  
  
    qn2<-nn.ff2(qn1,t(next_state))
    re_ep<-return_reward(next_state,current_state) ## get a reward and Whether the episode ends for action(next state)
    qv<-qn2$post[[length(qn2$size)]] 
    
    if(state_index==100){ ## if episonde done
      
      qv[action_index] <- re_ep[1]
      true_y<-qv
      
    }else{
  
      qv[action_index] <-  re_ep[1]+ dis_f * max(qv)
      true_y<- qv
    }
    qn1$e<-   true_y-qn1$post[[length(size)]]
    qn1$L[step]<- mean(qn1$e^2)
    
    
    
    ######## qnetwork backpropagation
    qn1<-nn.bp(qn1)
    
    
    ## total reward
    total_r<-total_r+re_ep[1] 
    
    ## episode end??
    episode_done<-re_ep[2]
    step<-step+1
    
    if(episode_done==1){
      
      cat("\n",i," epsode-",step)
      step_list<-c(step_list,step)
      final_action_list[[i]]<-action_list
      reward_list<-c(reward_list,total_r)
      cat("\n final location")
      print(coord(next_state))
      if(i %%10==0){
      ad<-apply(nn.ff2(qn1,diag(100))$post[[3]],1,which.max);ad
      q_table[[i]]<-matrix(action[ad],ncol=10,byrow=T)
      
  
      print(matrix(action[ad],ncol=10,byrow=T))
      

      ts.plot(reward_list,main=paste0((reward_list)[length(reward_list)],"-",step,"-",min(step_list)))
      }
      break;
    }
    
    
    ### max step
    if(step == 100){
      cat("\n final location")
      print(coord(next_state))
      step_list<-c(step_list,step)
      final_action_list[[i]]<-action_list
      reward_list<-c(reward_list,total_r)
      if(i %% 10 ==0){
      ad<-apply(nn.ff2(qn1,diag(100))$post[[3]],1,which.max);ad
      print(matrix(action[ad],ncol=10,byrow=T))
      
     
      ts.plot(reward_list,main=paste0((reward_list)[length(reward_list)],"-",step,"-",min(step_list)))
      }
        break;
      
    }
    
  }
  
}

ts.plot(step_list,main="step")
ts.plot(reward_list,main="Qnetwork-reward")
  
